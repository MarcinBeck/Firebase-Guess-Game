<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <title>Test Detekcji Twarzy</title>
    <style>
        body { font-family: sans-serif; background: #111; color: #fff; text-align: center; }
        .video-container { position: relative; width: fit-content; margin: 20px auto; }
        video, canvas { border-radius: 8px; }
        canvas { position: absolute; top: 0; left: 0; }
    </style>
</head>
<body>
    <h1>Minimalny Test Detekcji Twarzy</h1>
    <p id="status">Ładowanie modeli...</p>
    <div class="video-container">
        <video id="video" autoplay muted playsinline></video>
        <canvas id="overlay"></canvas>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-detection"></script>
    
    <script>
        const video = document.getElementById('video');
        const overlay = document.getElementById('overlay');
        const status = document.getElementById('status');
        const overlayCtx = overlay.getContext('2d');
        let faceDetector;

        async function runTest() {
            try {
                // 1. Załaduj model
                status.textContent = "Ładowanie modelu detekcji twarzy...";
                const model = faceDetection.SupportedModels.MediaPipeFaceDetector;
                const detectorConfig = { runtime: 'tfjs' };
                faceDetector = await faceDetection.createDetector(model, detectorConfig);
                status.textContent = "Model załadowany. Uruchamianie kamery...";

                // 2. Uruchom kamerę
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                await video.play();
                status.textContent = "Kamera aktywna. Rozpoczynam detekcję...";

                // 3. Uruchom pętlę detekcji
                video.addEventListener('loadeddata', () => {
                    overlay.width = video.videoWidth;
                    overlay.height = video.videoHeight;
                    detectFacesLoop();
                });

            } catch (e) {
                status.textContent = "WYSTĄPIŁ BŁĄD: " + e.message;
                console.error(e);
            }
        }

        async function detectFacesLoop() {
            if (faceDetector && !video.paused && !video.ended) {
                const faces = await faceDetector.estimateFaces(video, { flipHorizontal: false });
                
                overlayCtx.clearRect(0, 0, overlay.width, overlay.height);

                if (faces.length > 0) {
                    status.textContent = `Wykryto twarzy: ${faces.length}`;
                } else {
                    status.textContent = "Nie wykryto twarzy...";
                }
                
                faces.forEach(face => {
                    overlayCtx.strokeStyle = '#00FF00'; // Zielony dla odróżnienia
                    overlayCtx.lineWidth = 4;
                    overlayCtx.strokeRect(face.box.xMin, face.box.yMin, face.box.width, face.box.height);
                });

                requestAnimationFrame(detectFacesLoop);
            }
        }

        runTest();
    </script>
</body>
</html>
